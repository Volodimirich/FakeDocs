# Logging metrics
basic:
  wandb_project: "MadeFakeDocs"

# Model
model:
# "t5-v1_1-small", "t5-v1_1-large", "ai-forever/FRED-T5-1.7B", "gpt2" "ai-forever/rugpt3large_based_on_gpt2"
  model_name: "ai-forever/FRED-T5-1.7B"
  tokenizer_name: "ai-forever/FRED-T5-1.7B"
  local_path: './result_t5_test'
  use_local: False
  path_save_model: str
  input_max_length: 20
  target_max_length: 750
  total_samples: 1000000
  type_of_training: "TEACHER"


# Dataset
dataset:
  # file_path: 'D:\\projects_andrey\\datasets_made\\fulldocs_txt.txt'
  save_dir: '/home/d.maximov/data'
  # full_data - с сайта, part_data - размеченная с hugging face
  # made_data
  dataset_name: "made_data"
  block_size: 1024
  mlm: False
  path_to_save_txt: "./data/gpt_training/txt_training.txt"
  name_txt_file: "txt_training.txt"

  # Gdown помойка, надеюсь они починять folder_download
  gdrive_dict:
    full_data: https://drive.google.com/uc?id=1NCjkrgzKsOpwxHUMW-x0KQzSZbpm6g9P 
    part_data: 
      train: https://drive.google.com/uc?id=1x_Sho7xYqsJPvB8zDZWudSO4WsifhJfs
      test: https://drive.google.com/uc?id=1R-k_Osj7NFi1rEvx0citRIbzXT7IzIAc
      val: https://drive.google.com/uc?id=1RBoqgf2d5cGUNne6F7ZWZd4GUwQpSqjZ

# TrainingArguments
train_params:
  pre_trained: bool
  output_dir: "/home/d.maximov/data/models/t5_1.7B"
  overwrite_output_dir: True
  num_train_epochs: 40  # number of training epochs
  per_device_train_batch_size: 1  # batch size for training
  per_device_eval_batch_size: 1  # batch size for evaluation
  warmup_steps: 10  # number of warmup steps for learning rate scheduler
  gradient_accumulation_steps: 16   # to make "virtual" batch size larger

  # Optimizer
  lr: 1e-3
